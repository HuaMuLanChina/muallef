# Introduction {-}

Music is ubiquitous ever since humans exist.
Prehistoric instruments have been found and thought to be at least
40,000 years old.
Music is a pilar of human civilisation; it relates to people's identities,
feelings and thoughts.
Hence, means of saving and sharing music are of invaluable importance.
The oldest surviving notated music work *Hurrian Hymn to Nikkal*
found on clay tablets dates back to 1400 BC.

Various systems were developped around the globe for *visually*
representing perceived music through the use of written symbols.
The modern western notation is the predominent musical notation
worldwide for most music genres.

With the rise of technology, audio recordings where introduced
as analog signals and eventually as digital signals,
providing means for sharing and sauveguarding music *aurally*.

Music theory and musical notation have been studied for centuries,
allowing humans and machines to retrieve music information
from common formats.
Nevertheless, *music processing* is a relatively young discipline
compared to other subdomains of signal processing such as speech
processing; while great results are achieved today in speech recognition,
the task of retreiving music information from audio recordings
is still far along.

**Automatic Music Transcription** (AMT) is the task of analyzing
musical audio signals and producing the corresponding musical scores.
This task has captured researchers interest in the late 20^th^ century
and has become a wide research discipline as many of the problems
in this domain remain unsolved.
Furthurmore, strides in the domain of AMT would apply to numerous
applications that can facilitate creating, sharing, and learning music.

The scope of this thesis is the domain of Automatic Music Transcription
and the underlying tasks.
We explore the state of the art and propose an implementation
for a subset of the presented methods.

\pagebreak