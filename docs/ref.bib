
@article{benetos_2013,
	title = {Automatic music transcription: {Challenges} and future directions},
	volume = {41},
	shorttitle = {Automatic music transcription},
	doi = {10.1007/s10844-013-0258-3},
	abstract = {Automatic music transcription is considered by many to be a key enabling technology in music signal processing. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. One way to overcome the limited performance of transcription systems is to tailor algorithms to specific use-cases. Semi-automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information from multiple algorithms and different musical aspects.},
	journal = {Journal of Intelligent Information Systems},
	author = {Benetos, Emmanouil and Dixon, Simon and Giannoulis, Dimitrios and Kirchhoff, Holger and Klapuri, Anssi},
	month = dec,
	year = {2013},
	file = {Full Text PDF:/home/rand/Zotero/storage/4JRUEL5K/Benetos et al. - 2013 - Automatic music transcription Challenges and futu.pdf:application/pdf}
}

@book{brossier_automatic_2006,
	title = {Automatic {Annotation} of {Musical} {Audio} for {Interactive} {Applications}},
	author = {Brossier, Paul M.},
	year = {2006},
	file = {Citeseer - Snapshot:/home/rand/Zotero/storage/PNFK4EQD/summary.html:text/html;Citeseer - Full Text PDF:/home/rand/Zotero/storage/82FMYHF9/Brossier - 2006 - Automatic Annotation of Musical Audio for Interact.pdf:application/pdf}
}

@article{yeh_thesis,
	title = {Multiple {Fundamental} {Frequency} {Estimation} of {Polyphonic} {Recordings}},
	language = {en},
	author = {Yeh, Chunghsin},
	pages = {153},
	file = {Yeh - Multiple Fundamental Frequency Estimation of Polyp.pdf:/home/rand/Zotero/storage/HMBS4B73/Yeh - Multiple Fundamental Frequency Estimation of Polyp.pdf:application/pdf}
}

@article{goto_real-time_2004,
	title = {A real-time music-scene-description system: predominant-{F0} estimation for detecting melody and bass lines in real-world audio signals},
	volume = {43},
	issn = {01676393},
	shorttitle = {A real-time music-scene-description system},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639304000640},
	doi = {10.1016/j.specom.2004.07.001},
	abstract = {In this paper, we describe the concept of music scene description and address the problem of detecting melody and bass lines in real-world audio signals containing the sounds of various instruments. Most previous pitch-estimation methods have had diﬃculty dealing with such complex music signals because these methods were designed to deal with mixtures of only a few sounds. To enable estimation of the fundamental frequency (F0) of the melody and bass lines, we propose a predominant-F0 estimation method called PreFEst that does not rely on the unreliable fundamental component and obtains the most predominant F0 supported by harmonics within an intentionally limited frequency range. This method estimates the relative dominance of every possible F0 (represented as a probability density function of the F0) by using MAP (maximum a posteriori probability) estimation and considers the F0Õs temporal continuity by using a multiple-agent architecture. Experimental results with a set of ten music excerpts from compact-disc recordings showed that a real-time system implementing this method was able to detect melody and bass lines about 80\% of the time these existed.},
	language = {en},
	number = {4},
	urldate = {2020-03-05},
	journal = {Speech Communication},
	author = {Goto, Masataka},
	month = sep,
	year = {2004},
	pages = {311--329},
	file = {Goto - 2004 - A real-time music-scene-description system predom.pdf:/home/rand/Zotero/storage/6MJL2ZRG/Goto - 2004 - A real-time music-scene-description system predom.pdf:application/pdf}
}

@article{de_cheveigne_multiple_1999,
	title = {Multiple period estimation and pitch perception model},
	volume = {27},
	issn = {0167-6393},
	url = {http://www.sciencedirect.com/science/article/pii/S0167639398000740},
	doi = {10.1016/S0167-6393(98)00074-0},
	abstract = {The pitch of a periodic sound is strongly correlated with its period. To perceive the multiple pitches evoked by several simultaneous sounds, the auditory system must estimate their periods. This paper proposes a process in which the periodic sounds are canceled in turn (multistep cancellation model) or simultaneously (joint cancellation model). As an example of multistep cancellation, the pitch perception model of Meddis and Hewitt, 1991a, Meddis and Hewitt, 1991b can be associated with the concurrent vowel identification model of Meddis and Hewitt (1992). A first period estimate is used to suppress correlates of the dominant sound, and a second period is then estimated from the remainder. The process may be repeated to estimate further pitches, or else to recursively refine the initial estimates. Meddis and Hewitt's models are spectrotemporal (filter channel selection based on temporal cues) but multistep cancellation can also be performed in the spectral or time domain. In the joint cancellation model, estimation and cancellation are performed together in the time domain: the parameter space of several cascaded cancellation filters is searched exhaustively for a minimum output. The parameters that yield this minimum are the period estimates. Joint cancellation is guaranteed to find all periods, except in certain situations for which the stimulus is inherently ambiguous.
Résumé
La hauteur d'un son périodique est étroitement liée à sa période. Pour percevoir les hauteurs multiples de plusieurs sons simultanés, le système auditif doit estimer leurs périodes. Cet article propose un processus d'estimation par lequel les sons périodiques sont annulés les uns après les autres (modèle d'annulation successive), ou simultanément (modèle d'annulation simultanée). Comme exemple de modèle d'annulation successive, le modèle de perception de la hauteur de Meddis et Hewitt (1991a,b) peut être associé au modèle d'identification de voyelles concurrentes de Meddis et Hewitt (1992). Une première estimation de la période sert à supprimer les corrélats du premier son, et la deuxième période est estimée à partir du reste de cette opération. L'opération peut être répétée pour estimer d'autres périodes, ou pour affiner les estimations initiales de façon récursive. Les modèles de Meddis et Hewitt sont de type spectro-temporel (sélection de canaux de filtre selon des critères temporels), mais l'annulation successive peut s'opérer aussi bien dans le domaine temps ou fréquence. Dans le modèle d'annulation simultanée, estimation et annulation se font ensemble dans le domaine temps: l'espace des paramètres d'une cascade de filtres d'annulation est parcouru exhaustivement, à la recherche d'un minimum du signal de sortie. Les paramètres de ce minimum sont les estimations des périodes. Le succès du modè le d'annulation simultanée est guaranti, sauf dans certaines situations où le stimulus est ambigu.},
	language = {en},
	number = {3},
	urldate = {2020-03-06},
	journal = {Speech Communication},
	author = {de Cheveigné, Alain and Kawahara, Hideki},
	month = apr,
	year = {1999},
	keywords = {Computational auditory scene analysis, Fundamental frequency, Pitch perception, Speech analysis},
	pages = {175--185},
	file = {ScienceDirect Snapshot:/home/rand/Zotero/storage/YDEP3AVT/S0167639398000740.html:text/html;de Cheveigné and Kawahara - 1999 - Multiple period estimation and pitch perception mo.pdf:/home/rand/Zotero/storage/BR9ZBRLL/de Cheveigné and Kawahara - 1999 - Multiple period estimation and pitch perception mo.pdf:application/pdf}
}

@article{chunghsin_yeh_multiple_2010,
	title = {Multiple {Fundamental} {Frequency} {Estimation} and {Polyphony} {Inference} of {Polyphonic} {Music} {Signals}},
	volume = {18},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/5200519/},
	doi = {10.1109/TASL.2009.2030006},
	abstract = {This article presents a frame-based system for estimating multiple fundamental frequencies (F0s) of polyphonic music signals based on the STFT (short-time Fourier transform) representation. To estimate the number of sources along with their F0s, it is proposed to estimate the noise level beforehand and then jointly evaluate all the possible combinations among pre-selected F0 candidates. Given a set of F0 hypotheses, their hypothetical partial sequences are derived, taking into account where partial overlap may occur. A score function is used to select the plausible sets of F0 hypotheses. To infer the best combination, hypothetical sources are progressively combined and iteratively veriﬁed. A hypothetical source is considered valid if it either explains more energy than the noise, or improves signiﬁcantly the envelope smoothness once the overlapping partials are treated. The proposed system has been submitted to MIREX (Music Information Retrieval Evaluation eXchange) 2007 and 2008 contests where the accuracy has been evaluated with respect to the number of sources inferred and the precision of the F0s estimated. The encouraging results demonstrate its competitive performance among the state-of-the-art methods.},
	language = {en},
	number = {6},
	urldate = {2020-03-06},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {{Chunghsin Yeh} and Roebel, Axel and Rodet, Xavier},
	month = aug,
	year = {2010},
	pages = {1116--1126},
	file = {Chunghsin Yeh et al. - 2010 - Multiple Fundamental Frequency Estimation and Poly.pdf:/home/rand/Zotero/storage/UN98Y99Q/Chunghsin Yeh et al. - 2010 - Multiple Fundamental Frequency Estimation and Poly.pdf:application/pdf}
}

@article{yin_2002,
	title = {{YIN}, a fundamental frequency estimator for speech and music},
	volume = {111},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/10.1121/1.1458024},
	doi = {10.1121/1.1458024},
	number = {4},
	urldate = {2020-03-06},
	journal = {The Journal of the Acoustical Society of America},
	author = {de Cheveigné, Alain and Kawahara, Hideki},
	month = apr,
	year = {2002},
	pages = {1917--1930},
	file = {Snapshot:/home/rand/Zotero/storage/FFLXQKIS/1.html:text/html;Submitted Version:/home/rand/Zotero/storage/2LI8UADG/de Cheveigné and Kawahara - 2002 - YIN, a fundamental frequency estimator for speech .pdf:application/pdf}
}

@article{ross_average_1974,
	title = {Average magnitude difference function pitch extractor},
	volume = {22},
	issn = {0096-3518},
	doi = {10.1109/TASSP.1974.1162598},
	abstract = {This paper describes a method for using the average magnitude difference function (AMDF) and associated decision logic to estimate the pitch period of voiced speech sounds. The AMDF is a variation on autocorrelation analysis where, instead of correlating the input speech at various delays (where multiplications and summations are formed at each value of delay), a difference signal is formed between the delayed speech and the original and, at each delay, the absolute magnitude of the difference is taken. The difference signal is always zero at delay = π, and exhibits deep nulls at delays corresponding to the pitch period of voiced sounds. Some of the reasons the AMDF is attractive include the following. 1) It is a simple measurement which gives a good estimate of pitch contour, 2) it has no multiply operations, 3) its dynamic range characteristics are suitable for implementation on a 16-bit machine, and 4) the nature of its operations makes it suitable for implementation on a programmable processor or in special purpose hardware. The implementation of the AMDF pitch extractor (nonreal-time simulation and real-time) is described and experimental results presented to illustrate its basic measurement properties.},
	number = {5},
	journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
	author = {Ross, M. and Shaffer, H. and Cohen, A. and Freudberg, R. and Manley, H.},
	month = oct,
	year = {1974},
	keywords = {Speech analysis, Autocorrelation, Data processing, Delay, Dynamic range, Filters, Humans, Logic, Signal analysis, Speech synthesis},
	pages = {353--362},
	file = {IEEE Xplore Abstract Record:/home/rand/Zotero/storage/G65SNGXD/1162598.html:text/html;Ross et al. - 1974 - Average magnitude difference function pitch extrac.pdf:/home/rand/Zotero/storage/D4YHAU8P/Ross et al. - 1974 - Average magnitude difference function pitch extrac.pdf:application/pdf}
}

@article{lahat_spectral_1987,
	title = {A spectral autocorrelation method for measurement of the fundamental frequency of noise-corrupted speech},
	doi = {10.1109/TASSP.1987.1165224},
	abstract = {A method for measurement of the fundamental frequency of a voiced speech signal corrupted by high levels of additive white Gaussian noise is described. The method is based on flattening the spectrum of the signal by a bank of bandpass lifters and extracting the pitch frequency from autocorrelation functions calculated at the output of the lifters. A smoothing modified median filter is applied to the calculated pitch frequency contour to result in an improvement in the accuracy of the method. A byproduct of the pitch tracker is a voiced/ unvoiced classifier. The maximum and the variance of the autocorrelation function maxima, over the bank of lifters, serve as the basis for voiced/unvoiced classification by making use of a two-dimensional, nearest-neighbor pattern recognition approach. Results are presented for fundamental frequency measurement and voiced/unvoiced classification for several signal-to-noise ratios.},
	journal = {IEEE Trans. Acoustics, Speech, and Signal Processing},
	author = {Lahat, M. and Niederjohn, Russell J. and Krubsack, David A.},
	year = {1987},
	file = {Lahat et al. - 1987 - A spectral autocorrelation method for measurement .pdf:/home/rand/Zotero/storage/P68WM637/Lahat et al. - 1987 - A spectral autocorrelation method for measurement .pdf:application/pdf}
}

@article{noll_cepstrum_1967,
	title = {Cepstrum {Pitch} {Determination}},
	volume = {41},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/abs/10.1121/1.1910339},
	doi = {10.1121/1.1910339},
	number = {2},
	urldate = {2020-03-06},
	journal = {The Journal of the Acoustical Society of America},
	author = {Noll, A. Michael},
	month = feb,
	year = {1967},
	pages = {293--309},
	file = {Snapshot:/home/rand/Zotero/storage/24XKV4KR/1.html:text/html;Noll - 1967 - Cepstrum Pitch Determination.pdf:/home/rand/Zotero/storage/FPVJLR38/Noll - 1967 - Cepstrum Pitch Determination.pdf:application/pdf}
}

@article{schroeder_period_1968,
	title = {Period histogram and product spectrum: new methods for fundamental-frequency measurement.},
	shorttitle = {Period histogram and product spectrum},
	doi = {10.1121/1.1910902},
	abstract = {The fundamental frequency of a periodic signal whose fundamental component is not available for measurement can be determined by measuring the frequencies of its higher harmonic components and computing the largest common divider of these frequencies. Similarly, the fundamental period can be determined by measuring the periods of individual harmonics and finding their smallest common multiple. Several methods of fundamental frequency and period measurement, based on these concepts, are described in this paper. The results of computer simulations and analog instrumentations indicate that these new methods. at a considerable reduction in complexity, compare favorably with, and in some cases exceed, the capabilities of cepstrum analysis.},
	journal = {The Journal of the Acoustical Society of America},
	author = {Schroeder, Manfred R.},
	year = {1968},
	file = {Schroeder - 1968 - Period histogram and product spectrum new methods.pdf:/home/rand/Zotero/storage/72U5CIUE/Schroeder - 1968 - Period histogram and product spectrum new methods.pdf:application/pdf}
}

@article{oppenheim_frequency_2004,
	title = {From frequency to quefrency: a history of the cepstrum},
	volume = {21},
	issn = {1558-0792},
	shorttitle = {From frequency to quefrency},
	doi = {10.1109/MSP.2004.1328092},
	abstract = {The idea of the log spectrum or cepstral averaging has been useful in many applications such as audio processing, speech processing, speech recognition, and echo detection for the estimation and compensation of convolutional distortions. To suggest what prompted the invention of the term cepstrum, this article narrates the historical and mathematical background that led to its discovery. The computations of earlier simple echo representations have shown that the spectrum representation domain results does not belong in the frequency or time domain. Bogert et al. (1963) chose to refer to it as quefrency domain and later termed the spectrum of the log of a time waveform as the cepstrum. The article also recounts the analysis of Al Oppenheim in relation to the cepstrum. It was in his theory for nonlinear signal processing, referred to as homomorphic systems, that the realization of the characteristic system of homomorphic convolution was reminiscent of the cepstrum. To retain both the relationship to the work of Bogart et al. and the distinction, the term power cepstrum was eventually applied to the nonlinear mapping in homomorphic deconvolution . While most of the terms in the glossary have faded into the background, the term cepstrum has survived and has become part of the digital signal processing lexicon.},
	number = {5},
	journal = {IEEE Signal Processing Magazine},
	author = {Oppenheim, A.V. and Schafer, R.W.},
	month = sep,
	year = {2004},
	keywords = {Signal processing, cepstral analysis, Cepstral analysis, cepstrum, Cepstrum, Convolution, convolutional distortions, Deconvolution, digital signal processing lexicon, Frequency, History, homomorphic systems, log spectrum, Nonlinear distortion, nonlinear signal processing, signal processing, Speech processing, Speech recognition, time waveform},
	pages = {95--106},
	file = {IEEE Xplore Abstract Record:/home/rand/Zotero/storage/XVH62WJU/1328092.html:text/html;Oppenheim and Schafer - 2004 - From frequency to quefrency a history of the ceps.pdf:/home/rand/Zotero/storage/6I7YUVCF/Oppenheim and Schafer - 2004 - From frequency to quefrency a history of the ceps.pdf:application/pdf}
}

@misc{noauthor_feynman_nodate,
	title = {The {Feynman} {Lectures} on {Physics} {Vol}. {I} {Ch}. 47: {Sound}. {The} wave equation},
	url = {https://www.feynmanlectures.caltech.edu/I_47.html},
	urldate = {2020-03-08},
	file = {The Feynman Lectures on Physics Vol. I Ch. 47\: Sound. The wave equation:/home/rand/Zotero/storage/QDYKH6FK/I_47.html:text/html}
}

@misc{wiki:nyquistshannon,
	title = {Nyquist–{Shannon} sampling theorem},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Nyquist%E2%80%93Shannon_sampling_theorem&oldid=941933031},
	abstract = {In the field of digital signal processing, the sampling theorem is a fundamental bridge between continuous-time signals and discrete-time signals. It establishes a sufficient condition for a sample rate that permits a discrete sequence of samples to capture all the information from a continuous-time signal of finite bandwidth.
Strictly speaking, the theorem only applies to a class of mathematical functions having a Fourier transform that is zero outside of a finite region of frequencies. Intuitively we expect that when one reduces a continuous function to a discrete sequence and interpolates back to a continuous function, the fidelity of the result depends on the density (or sample rate) of the original samples. The sampling theorem introduces the concept of a sample rate that is sufficient for perfect fidelity for the class of functions that are bandlimited to a given bandwidth, such that no actual information is lost in the sampling process. It expresses the sufficient sample rate in terms of the bandwidth for the class of functions. The theorem also leads to a formula for perfectly reconstructing the original continuous-time function from the samples.
Perfect reconstruction may still be possible when the sample-rate criterion is not satisfied, provided other constraints on the signal are known. (See § Sampling of non-baseband signals below and compressed sensing.) In some cases (when the sample-rate criterion is not satisfied), utilizing additional constraints allows for approximate reconstructions. The fidelity of these reconstructions can be verified and quantified utilizing Bochner's theorem.The name Nyquist–Shannon sampling theorem honours Harry Nyquist and Claude Shannon although it had already been discovered in 1933 by Vladimir Kotelnikov. The theorem was also discovered independently by E. T. Whittaker and by others. It is thus also known by the names Nyquist–Shannon–Kotelnikov, Whittaker–Shannon–Kotelnikov, Whittaker–Nyquist–Kotelnikov–Shannon, and cardinal theorem of interpolation.},
	language = {en},
	urldate = {2020-03-08},
	journal = {Wikipedia},
	month = feb,
	year = {2020},
	note = {Page Version ID: 941933031},
	file = {Snapshot:/home/rand/Zotero/storage/I3RNMMYI/index.html:text/html}
}

@article{salamon_melody_nodate,
	title = {Melody {Extraction} from {Polyphonic} {Music} {Signals} using {Pitch} {Contour} {Characteristics}},
	abstract = {We present a novel system for the automatic extraction of the main melody from polyphonic music recordings. Our approach is based on the creation and characterisation of pitch contours, time continuous sequences of pitch candidates grouped using auditory streaming cues. We deﬁne a set of contour characteristics and show that by studying their distributions we can devise rules to distinguish between melodic and non-melodic contours. This leads to the development of new voicing detection, octave error minimisation and melody selection techniques.},
	language = {en},
	journal = {IEEE TRANSACTIONS ON AUDIO},
	author = {Salamon, Justin and Gomez, Emilia},
	pages = {12},
	file = {Salamon and Gomez - Melody Extraction from Polyphonic Music Signals us.pdf:/home/rand/Zotero/storage/U2AYQ8Z3/Salamon and Gomez - Melody Extraction from Polyphonic Music Signals us.pdf:application/pdf}
}